[{
    "title": "Copying Google Analytics Tables in BigQuery",
    "date": "",
    "description": "If you need to copy multiple date-sharded tables from one dataset to another in BigQuery, here's a script to automate it.",
    "body": "If you\u0026rsquo;ve ever needed to copy tables from one project or dataset to another in BigQuery, the below script may be a good option for you.\nThere are many options for copying tables in BigQuery, ranging from completely non-technical (point and click in the BigQuery UI) to more technical (command line and/or Python scripts).\nI won\u0026rsquo;t go into the pros and cons of each option, or why you would want to use one over the other. For a good explanation of that, this article from Vamsi Namburu does an excellent job at covering those details.\nBelow is the code I used to copy a range of daily tables from one dataset to another (can be the same project or a different project, as long as you have the right roles/permissions in each project). If you want to copy every single table, this is not a good option. In that case, you can just copy the entire dataset using the BQ UI, like pictured below:\n  Copying every table in a dataset into another project/dataset is both quick, free, and easy in the interface. Just click on \u0026lsquo;Copy Dataset\u0026rsquo; and specify the destination project and dataset.\n  In my particular use case, however, I needed to copy just a specific range of tables - not the entire dataset. For that, I wrote a script using data definition language (DDL). I\u0026rsquo;ve done my best to generalize the script to be as plug-and-play as possible. All you have to do is update the first few lines to specify your source and destination projects and datasets, as well as the start and end dates for the Google Analytics daily tables you want to copy.\n-------------------------------------------------------------- -- Copy multiple date-sharded GA tables to new project/dataset --------------------------------------------------------------  -- Update the variables below to specify your source project and dataset (where -- you want to copy the tables from), the destination project/dataset (where you -- want to copy the tables to), and the first and last date. The first and last -- date specify the date range (inclusive) of tables you want to copy. DECLARE source_project STRING DEFAULT \u0026#34;PROJECT_NAME\u0026#34;; DECLARE source_dataset STRING DEFAULT \u0026#34;DATASET_ID\u0026#34;; DECLARE destination_project STRING DEFAULT \u0026#34;PROJECT_NAME\u0026#34;; DECLARE destination_dataset STRING DEFAULT \u0026#34;DATASET_ID\u0026#34;; DECLARE start_date INT64 DEFAULT 20200625; DECLARE end_date INT64 DEFAULT 20200627; -- DO NOT TOUCH ANYTHING BELOW THIS LINE -- DECLARE source_ga_tables_array ARRAY\u0026lt;STRING\u0026gt;; DECLARE destination_ga_tables_array ARRAY\u0026lt;STRING\u0026gt;; DECLARE source_ga_table STRING; DECLARE destination_ga_table STRING; DECLARE i INT64 DEFAULT 0; SET source_ga_tables_array = ( WITH date_array AS ( SELECT GENERATE_ARRAY(start_date, end_date) AS dates ) SELECT ARRAY_AGG( CONCAT(source_project, \u0026#39;.\u0026#39;, source_dataset, \u0026#39;.ga_sessions_\u0026#39;, date) ) FROM date_array, UNNEST(dates) AS date ); SET destination_ga_tables_array = ( SELECT ARRAY_AGG( REGEXP_REPLACE(source_ga_tables_array, CONCAT(source_project, \u0026#39;.\u0026#39;, source_dataset), CONCAT(destination_project, \u0026#39;.\u0026#39;, destination_dataset) ) ) AS table_name FROM UNNEST(source_ga_tables_array) AS source_ga_tables_array ); WHILE i \u0026lt; ARRAY_LENGTH(source_ga_tables_array) DO SET source_ga_table = ( SELECT CONCAT(\u0026#39;`\u0026#39;, source_ga_tables_array[OFFSET(i)], \u0026#39;`\u0026#39;) ); SET destination_ga_table = ( SELECT CONCAT(\u0026#39;`\u0026#39;, destination_ga_tables_array[OFFSET(i)], \u0026#39;`\u0026#39;) ); EXECUTE IMMEDIATE format(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE %s AS SELECT * FROM %s \u0026#34;\u0026#34;\u0026#34;, destination_ga_table, source_ga_table); SET i = i + 1; END WHILE; ",
    "ref": "/blog/copying-tables-in-bigquery/"
  },{
    "title": "Updating Google Analytics Hit-Level Custom Dimensions in BigQuery",
    "date": "",
    "description": "Updating hit-level custom dimension information in BigQuery is not as simple as it sounds. Here's the code to do it.",
    "body": "I recently had a client with a problem. There was an issue with their Google Tag Manager implementation that prevented the collection of certain hit-level custom dimensions to Google Analytics.\nFortunately, they had GA360 (with the export to BigQuery already enabled), and the missing custom dimension information was able to be replicated based on the page path. In other words, I could create a lookup table table based on the page path of the hit to grab the corresponding custom dimension value (in this particular case, it was a custom dimension named Site ID).\nIt seemed simple enough. But it turned out to be rather complex. (If you know of a simpler soultion, please let me know! I do sometimes have a tendency to find overly complicated solutions.)\nThe complexity stems from the fact that these are hit-level custom dimensions, requiring me to have to UNNEST two levels deep (UNNEST the hits, then UNNEST the custom dimensions).\nAlso, I wanted to use the BigQuery data manipulation language (DML) to do the updates - mainly because I haven\u0026rsquo;t had a chance to really use it before and also because it seemed appropriate for the task.\nWith DML, you can update, insert, and delete data from your tables. I needed to update the tables, but specifically, I needed to update the hits field. Due to the nested nature of the Google Analytics data in BigQuery, the hits field is a single column (comprised of 176 other \u0026ldquo;sub-fields\u0026rdquo;!).\nI thought I\u0026rsquo;d be able to update just part of the hit (specifically, the value for custom dimension index 1). However, almost every method I tried ended in the following error:\nError: Correlated subqueries that reference other tables are not supported unless they can be de-correlated, such as by transforming them into an efficient JOIN.\nAfter spending too many hours on Stack Overflow, reading BigQuery documentation on ARRAY and STRUCT, and experimenting with different queries, I came to the realization that I\u0026rsquo;d just have to UNNEST the hits and the hits.customDimensions, join on the lookup table table, then recreate the the entire hits field for each session. Fun.\nFor those that want to follow along, below is the SQL to generate a couple rows to mimic Google Analytics data in BQ. This is a simpler dataset to work with, which I found helps when trying to tackle larger problems.\n-- Create a table to mimic a session in GA SELECT \u0026#39;1234\u0026#39; AS clientId, 12345678 AS visitStartTime, STRUCT(1 AS visits, 2 AS hits) AS totals, [ STRUCT(1 AS hitNumber, STRUCT(\u0026#39;/careers/\u0026#39; AS pagePath, \u0026#39;example.com\u0026#39; AS hostname) AS page, [STRUCT(1 AS index, \u0026#39;one\u0026#39; AS value), STRUCT(2 AS index, \u0026#39;two\u0026#39; AS value)] AS customDimensions), STRUCT(2 AS hitNumber, STRUCT(\u0026#39;/insights/\u0026#39; AS pagePath, \u0026#39;example.com\u0026#39; AS hostname) AS page, [STRUCT(1 AS index, \u0026#39;(not set)\u0026#39; AS value), STRUCT(2 AS index, \u0026#39;two-more\u0026#39; AS value)] AS customDimensions) ] AS hits UNION ALL SELECT \u0026#39;9877\u0026#39; AS clientId, 23456789 AS visitStartTime, STRUCT(1 AS visits, 3 AS hits) AS totals, [ STRUCT(1 AS hitNumber, STRUCT(\u0026#39;/\u0026#39; AS pagePath, \u0026#39;example2.com\u0026#39; AS hostname) AS page, [STRUCT(1 AS index, \u0026#39;one\u0026#39; AS value), STRUCT(2 AS index, \u0026#39;two\u0026#39; AS value)] AS customDimensions), STRUCT(2 AS hitNumber, STRUCT(\u0026#39;/blog/\u0026#39; AS pagePath, \u0026#39;example2.com\u0026#39; AS hostname) AS page, [STRUCT(2 AS index, \u0026#39;dos\u0026#39; AS value)] AS customDimensions), STRUCT(3 AS hitNumber, STRUCT(\u0026#39;/blog/post/\u0026#39; AS pagePath, \u0026#39;example2.com\u0026#39; AS hostname) AS page, [STRUCT(1 AS index, \u0026#39;un\u0026#39; AS value), STRUCT(2 AS index, \u0026#39;deux\u0026#39; AS value)] AS customDimensions) ] AS hits   The above SQL will ouput two rows, with nested hits and nested custom dimensions.\n  Notice in the screenshot above how the second hit of the first row (with a hits.page.pagePath of /insights/) has a value of (not set) for custom dimension index 1. The goal is to update that value from (not set) to a value based on a separate lookup table table.\nThe following SQL outputs our sample lookup table table:\n-- Create a table to mimic a lookup table table with pagePath and site ID SELECT \u0026#39;/careers/\u0026#39; AS pagePath, \u0026#39;C-3PO\u0026#39; AS site_id UNION ALL SELECT \u0026#39;/\u0026#39; AS pagePath, \u0026#39;R2-D2\u0026#39; AS site_id UNION ALL SELECT \u0026#39;/insights/\u0026#39; AS pagePath, \u0026#39;867-5309\u0026#39; AS site_id   This is our lookup table table. Whenever custom dimension index 1 has a value of (not set), we\u0026rsquo;ll replace that with the value in this table, based on the page path of the hit.\n  Given the sample table generated by the first bit of SQL, we see that the second hit of the first session has a value of (not set) for custom dimension index 1 and a hits.page.pagePath of /insights/. So we\u0026rsquo;d join with the lookup table table where hits.page.pagePath = pagePath, and replace (not set) with 867-5309 - the value of the site_id field in the lookup table table.\nTo follow along with this example, run the queries above and save the results as tables. I named them ga_sessions_sample and site_id_lookup, which I\u0026rsquo;ll reference throughout the rest of this article.\nTime to Transform Now that we\u0026rsquo;ve set up some sample tables, it\u0026rsquo;s time to get to work. There are basically 2 steps that need to happen:\nStep 1: Create an \u0026ldquo;updated hits\u0026rdquo; table with clientId, visitStartTime, and the entire hits field (with the custom dimension value updated). This will be the table that we use to replace the hits field in the original ga_sessions_* tables.\nStep 2: Run through the DML to update the hits field in the ga_sessions_* tables with the hits in our updated table.\nStep 1 This query will update every instance of custom dimension Index 1 where the value is (not set), using the lookup table table to replace (not set) with the site_id value. Run this query and save the output to a table (I named this table updated_hits)\n-- t1 UNNESTs the table and joins on the lookup table to get the correct value for Site ID WITH t1 AS ( SELECT clientId , visitStartTime , hits , cd.index , IF(cd.index = 1 AND cd.value = \u0026#39;(not set)\u0026#39;, site_id, cd.value) AS value FROM `PROJECT.DATASET.ga_sessions_sample` AS sessions, UNNEST(hits) AS hits, UNNEST(hits.customDimensions) AS cd LEFT JOIN `PROJECT.DATASET.site_id_lookup` AS site_id_lookup ON hits.page.pagePath = site_id_lookup.pagePath ), -- t2 puts the CD index and value into an array of structs, and selects all other fields t2 AS ( SELECT * EXCEPT(hits, index, value) , hits.hitNumber , ANY_VALUE(hits.page) AS page , ARRAY_AGG(STRUCT(index, value)) AS customDimensions FROM t1 GROUP BY clientId, visitStartTime, hitNumber ORDER BY clientId, visitStartTime, hitNumber ASC ) -- forms all the hit fields into an array of structs  SELECT clientId , visitStartTime , ARRAY_AGG(STRUCT(hitNumber, page, customDimensions)) AS hits FROM t2 GROUP BY clientId, visitStartTime   This is our updated hits table. It includes just the clientId, visitStartTime, and hits fields (with the updated custom dimension value).\n  Now that you have a table with the clientId and visitStartTime (which we use to uniquely identify a session), along with the updated hits field for that session (which includes the updated custom dimension value), all that\u0026rsquo;s left is a simple DML script to update the original ga_sessions_sample table:\nUPDATE `PROJECT.DATASET.ga_sessions_sample` AS a SET a.hits = b.hits FROM `PROJECT.DATASET.updated_hits` AS b WHERE a.clientId = b.clientId AND a.visitStartTime = b.visitStartTime After running the above script, check your ga_sessions_sample table again. You should see something like below:\n  This is our updated ga_sessions_sample table. It includes all of the original fields, with the updated custom dimension value outlined in red).\n  Play Time\u0026rsquo;s Over Now that we have the proof of concept working on a sample dataset, it\u0026rsquo;s time to apply the same principles (and make some slight modifications) to work with multiple days of real GA data.\nHere\u0026rsquo;s the SQL to update and recreate the entire hits record:\n-- t1 UNNESTs the table and joins on the lookup table to get the correct value for SiteID WITH t1 AS ( SELECT clientId , visitStartTime , hits , cd.index , CASE WHEN cd.index = 1 AND cd.value = \u0026#39;(not set)\u0026#39; THEN site_id ELSE cd.value END AS value FROM `PROJECT.DATASET.ga_sessions_*` AS sessions, UNNEST(hits) AS hits, UNNEST(hits.customDimensions) AS cd LEFT JOIN `PROJECT.DATASET.site_id_lookup` AS lookup_table ON lookup_table.pagePath = hits.page.pagePath WHERE _TABLE_SUFFIX BETWEEN \u0026#39;20201001\u0026#39; AND \u0026#39;20201007\u0026#39; ), -- t2 puts the CD index and value into an array of structs, and selects all other fields t2 AS ( SELECT * EXCEPT(hits, index, value) , hits.* EXCEPT(page , customDimensions , transaction , item , contentInfo , appInfo , exceptionInfo , eventInfo , product , promotion , promotionActionInfo , refund , eCommerceAction , experiment , publisher , customVariables , customMetrics , social , latencyTracking , sourcePropertyInfo , contentGroup , publisher_infos) , ANY_VALUE(hits.page) AS page , ANY_VALUE(hits.transaction) AS transaction , ANY_VALUE(hits.item) AS item , ANY_VALUE(hits.contentInfo) AS contentInfo , ANY_VALUE(hits.appInfo) AS appInfo , ANY_VALUE(hits.exceptionInfo) AS exceptionInfo , ANY_VALUE(hits.eventInfo) AS eventInfo , ANY_VALUE(hits.product) AS product , ANY_VALUE(hits.promotion) AS promotion , ANY_VALUE(hits.promotionActionInfo) AS promotionActionInfo , ANY_VALUE(hits.refund) AS refund , ANY_VALUE(hits.ecommerceAction) AS ecommerceAction , ANY_VALUE(hits.experiment) AS experiment , ANY_VALUE(hits.publisher) AS publisher , ANY_VALUE(hits.customVariables) AS customVariables , ANY_VALUE(hits.customMetrics) AS customMetrics , ANY_VALUE(hits.social) AS social , ANY_VALUE(hits.latencyTracking) AS latencyTracking , ANY_VALUE(hits.sourcePropertyInfo) AS sourcePropertyInfo , ANY_VALUE(hits.contentGroup) AS contentGroup , ANY_VALUE(hits.publisher_infos) AS publisher_infos , ARRAY_AGG(STRUCT(index, value)) AS customDimensions FROM t1 GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13 ORDER BY clientId, visitStartTime, hitNumber ASC ) -- forms all the hit fields into an array of structs  SELECT clientId , visitStartTime , ARRAY_AGG(STRUCT( hitNumber , time , hour , minute , isSecure , isInteraction , isEntrance , isExit , referer , page , transaction , item , contentInfo , appInfo , exceptionInfo , eventInfo , product , promotion , promotionActionInfo , refund , ecommerceAction , experiment , publisher , customVariables , customDimensions , customMetrics , type , social , latencyTracking , sourcePropertyInfo , contentGroup , dataSource , publisher_infos)) AS hits FROM t2 GROUP BY clientId, visitStartTime 114 lines of SQL isn\u0026rsquo;t bad, considering there are 176 different fields all nested within the hits record.\nRunning the above script will output the \u0026ldquo;updated_hits,\u0026rdquo; with the clientID, visitStartTime, and (updated) hits fields. This is what we did in Step 1 above. Again, you\u0026rsquo;ll want to save the output of this query to a table (I named it updated_hits).\nUnfortunately, if your date range is too long or if you have a lot of data, the above query will produce the following error:   Uh oh - looks like we got a little crazy with our query and BQ can\u0026rsquo;t handle it!\n  This is the result of several computationally expensive operations, starting with the multiple UNNESTs in the first common table expression (CTE), followed by horrendous 1-2 punch of a 13-field GROUP BY and 3-field ORDER BY - ouch!\nI haven\u0026rsquo;t found an elegant solution around this yet, and I had a bit of a time crunch to get this out for the client. So I\u0026rsquo;m somewhat ashamed to admit that I just brute-forced my way through this by shortening the date range (to no more than 3 days as a time) and kept running this query over and over again (for about an hour and a half). I was appending the results to a single \u0026ldquo;updated hits\u0026rdquo; table each time. I\u0026rsquo;m not proud.\nFortunately, once I suffered through the manual updates, the rest was easily automated. I just repurposed the DML statement from above and sprinkled in some variables and a WHILE loop. This way, it loops through each ga_sessions_* table and replaces the hits field from the updated table.\n-- Update the first 7 variables to reflect your project, dataset, and table names and the -- start and end dates of the tables you want to update. DECLARE original_ga_sessions_project STRING DEFAULT \u0026#34;PROJECT\u0026#34;; DECLARE original_ga_sessions_dataset STRING DEFAULT \u0026#34;DATASET\u0026#34;; DECLARE updated_table_project STRING DEFAULT \u0026#34;PROJECT\u0026#34;; DECLARE updated_table_dataset STRING DEFAULT \u0026#34;DATASET\u0026#34;; DECLARE updated_table STRING DEFAULT \u0026#34;TABLE_NAME\u0026#34;; DECLARE first_date INT64 DEFAULT 20200625; DECLARE last_date INT64 DEFAULT 20200627; -- DO NOT TOUCH ANYTHING BELOW THIS LINE -- DECLARE original_ga_tables_array ARRAY\u0026lt;STRING\u0026gt;; DECLARE original_ga_table STRING; DECLARE updated_hits STRING; DECLARE i INT64 DEFAULT 0; SET updated_hits = ( SELECT CONCAT(\u0026#39;`\u0026#39;, updated_table_project, \u0026#39;.\u0026#39;, updated_table_dataset, \u0026#39;.\u0026#39;, updated_table, \u0026#39;`\u0026#39;) ); SET original_ga_tables_array = ( WITH date_array AS ( SELECT GENERATE_ARRAY(first_date, last_date) AS dates ) SELECT ARRAY_AGG(CONCAT(original_ga_sessions_project, \u0026#39;.\u0026#39;, original_ga_sessions_dataset, \u0026#39;.ga_sessions_\u0026#39;, date)) FROM date_array, UNNEST(dates) AS date ); WHILE i \u0026lt; ARRAY_LENGTH(original_ga_tables_array) DO SET original_ga_table = ( SELECT CONCAT(\u0026#39;`\u0026#39;, original_ga_tables_array[OFFSET(i)], \u0026#39;`\u0026#39;) ); EXECUTE IMMEDIATE format(\u0026#34;\u0026#34;\u0026#34; UPDATE %s AS a SET a.hits = b.hits FROM %s AS b WHERE a.clientId = b.clientId AND a.visitStartTime = b.visitStartTime \u0026#34;\u0026#34;\u0026#34;, original_ga_table, updated_hits); SET i = i + 1; END WHILE; All you have to do is update the first 7 variables in the above script with your specific project, dataset, and table names, along with the start and end dates of the tables you want to update. Just be ready to wait a while for this script to run if you have a lot of tables to update. In my particular example, it took 1 hour and 3 minutes to update about 3 months' worth of tables.\n  My new record for longest time to process a query - 1 hour and 3 minutes!\n  But Jim, what about \u0026hellip; I know, I can already hear the question \u0026ldquo;I need to update a hit-level custom dimension that isn\u0026rsquo;t included at all in the data - how do I do that?\u0026rdquo; That\u0026rsquo;s a great question. When I worked through the client problem that inspired this post, I was fortunate (I think) that if the custom dimension didn\u0026rsquo;t have a value in the dataLayer, they were falling back to manually setting the value to (not set).\nThis makes the update fairly straight forward:\nIF(cd.index = 1 AND cd.value = '(not set)', site_id, cd.value) AS value\nBut if nothing at all is being passed, it gets tricky. I haven\u0026rsquo;t quite thought through that scenario, but if you\u0026rsquo;re interested in a solution and willing to buy me a drink at the next in-person conference once the plague lifts, let me know and I\u0026rsquo;ll figure it out for you.\n",
    "ref": "/blog/updating-hit-level-custom-dimensions-google-analytics/"
  },{
    "title": "About Jim Gianoglio",
    "date": "",
    "description": "",
    "body": "  Hi, I\u0026rsquo;m Jim Gianoglio. I\u0026rsquo;m a Lead Data Scientist at Bounteous, where I get to play with web analytics and digital marketing data every day.\nI graduated in 2018 from Carnegie Mellon University with a Master of Science in Information Technology (Business Intelligence \u0026amp; Data Analytics).\nI love to learn new things, and I love to teach. That\u0026rsquo;s one of the main reasons for this blog. It gives me an excuse to try new things, experiment a bit, and share what I\u0026rsquo;ve learned along the way.\nI\u0026rsquo;m always happy to chat - you can usually find me on Twitter or the Measure Slack.\n",
    "ref": "/about/"
  },{
    "title": "The beginning ...",
    "date": "",
    "description": "",
    "body": "  As the saying goes, a journey of a thousand miles begins with a single step. I don’t know if I have a thousand blog posts in me, but I have to start somewhere 🙂\nI plan to mostly write on topics related to analytics, data science, and machine learning. I’ve built up a lot of knowledge over the years on specific tools, like Google Analytics, Google Tag Manger, and BigQuery. So I imagine I’ll write about those tools and platforms specifically, in addition to other rants and thoughts on the broader, data-related topics.\nI might also venture off occasionally on completely unrelated topics of interest, like coffee or cycling.\nI’d be happy if you join me on this journey. Hopefully we’ll learn from each other along the way\u0026hellip;\n",
    "ref": "/blog/my-first-post/"
  }]
